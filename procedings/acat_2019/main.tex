% !TeX program = lualatex
% !TeX encoding = utf8
% !BIB program = bibtex

\documentclass[a4paper]{jpconf}
\usepackage{graphicx}
\usepackage{tikz}
\usepackage{units}
\usetikzlibrary{decorations.pathreplacing}

\bibliographystyle{iopart-num}

\begin{document}
\title{A hybrid deep learning approach to vertexing}

\author{
   Rui Fang\textsuperscript{1},
   Henry F Schreiner\textsuperscript{1,2},
   Mike D Sokoloff\textsuperscript{1},
   Constantin Weisser\textsuperscript{3} and
   Mike Williams\textsuperscript{3}
}

\address{
    \textsuperscript{1} University of Cincinnati, Cincinnati, OH, United States
}
\address{
    \textsuperscript{2} Princeton University, Princeton, NJ, United States
}
\address{
    \textsuperscript{3} Massachusetts Institute of Technology, Cambridge, MA, United States
}

\ead{hschrein@cern.ch}

\begin{abstract}
In the transition to Run 3 in 2021, LHCb will undergo a major luminosity upgrade, going from 1.1 to 5.6 expected visible Primary Vertices (PVs) per event, and will adopt a purely software trigger. This has fueled increased interest in alternative highly-parallel and GPU friendly algorithms for tracking and reconstruction. We will present a novel prototype algorithm for vertexing in the LHCb upgrade conditions.
We use a custom kernel to transform the sparse 3D space of hits and tracks into a dense 1D dataset, and then apply Deep Learning techniques to find PV locations. By training networks on our kernels using several Convolutional Neural Network layers, we have achieved better than 90\% efficiency with no more than 0.2 False Positives (FPs) per event. Beyond its physics performance, this algorithm also provides a rich collection of possibilities for visualization and study of 1D convolutional networks. We will discuss the design, performance, and future potential areas of improvement and study, such as possible ways to recover the full 3D vertex information.
\end{abstract}


\section{Introduction}

% TODO: Write this up

The LHCb detector is facing a major upgrade in luminosity in Run 3 in 2021. In each event, the Poisson average expected for the visible Primary Vertices (PVs) will go from 1.1 to 5.6. In addition, the hardware level-0 trigger is being removed in favor of a purely software trigger running at 30 MHz \cite{CERN-LHCC-2014-016}. Work is ongoing to find new algorithms to support the upgrade era software for LHCb.

The current algorithm for finding vertices is <REF NEEDED>.

An alternate method is proposed here for finding vertices from tracks or hits using machine learning techniques (see Figure \ref{fig:approach}). The algorithm starts with tracks containing location, direction, and covariance matrix information. The tracks in 3D space are converted to a 1D binned ``kernel'' through a process described below. Peaks in this kernel are closely related to the z positions of the primary verticies. We use a series of 1D convolutional layers to predict the PV locations from this kernel. The output probabilities from the neural network are converted to a list of PV candidate z-locations using a simple peak finding algorithm. Using this procedure we have surpassed 90\% efficency with less than 0.2 False Positives (FPs) per event.


\begin{figure}
	\centering
	\input{approach}
	\caption{The procedure followed in work to convert hits into primary vertices.}
	\label{fig:approach}
\end{figure}

A toy simulation of the LHCb detector has been developed to test this algorithm. The design of the algorithm and the toy that is was tested with follows.

\section{Kernel Generation}

Our toy simulation generates tracks from PVs and SVs, and we using a simple intersection procedure to produces hits in the Vertex Locater (VELO) according to a simple model of the upgrade detector \cite{Collaboration:1624070}. Pythia creates primary and secondary vertices in the beam interaction region. The particles it produces are then propogated through the detector, with interactions being recorded as hits in the 26 detection planes in the lower and upper halves of the VELO. Scattering in the detector and foil are also implemented.

The algorithm presented here applies to tracks, but our simulation produces raw hits, so a simple and inaccurate procedure is applied; this should be sufficient, since our algorithm is resilient to small tracking errors. We call this system ``prototracking'', and it was originally planned to be primary driver for the kernel generation to provide PV canidates as early as possible in the tracking procedure. Due to recent advancements in the tracking code, the full tracks will be available early in the trigger system, so the prototracking system will not be needed in the future. For the prototracking, the hits are radially sorted, and then formed into triplets, with a $\chi^2 < 10$ test for validity. Once a triplet is found, all hits that can be added to the track with $\chi^2<9$ are removed from further searches. This list is built and sent to the kernel.

In order to provide a simple, dense representation for the machine learning algorithm to use, the tracks are transformed into a dense representation using a ``kernel'' procedure. For any point in space, the value of the kernel can be computed as 

\begin{equation}
\rho \equiv  \frac{ \sum  p^2}{\sum  p} - \sum p
\end{equation}

where $p$ is Gaussian function in terms of distance to the impact parameter (IP) in x and y of a track, and is summed over all tracks.
In the future, a full covariant matrix will be used, but for now, a constant value for the Gaussian width of 0.05 is used, unless $\chi^2>6$, in which case $ (\chi^2-2)\frac{0.05}{4} $ is added to the width.
The final Gaussian is just the product of the Gaussians in $ \mathrm{IP} \cdot \hat{x}$ and $ \mathrm{IP} \cdot \hat{y}$.

A series of 4,000 regularly spaced planes along the $z$-axis (beamline) cover the active area of the VELO. Each plane is divided into a course 8 by 8 grid, and the kernel value is computed in the center of each bin. Once this is done, a standard minimization algorithm is used to find the maximum point starting from the center of the bin that reported the highest value; see Figure \ref{fig:kernel}. This maximum kernel value is recorded as the value for this plane in $z$, and the procedure is repeated for all 4000 planes.

Finally, the target for the training is generated from the simulated truth information. Each PV is represented by a normalized Gaussian with a fixed width of $\unit[100]{\mu m}$. These values are recorded into a 4000 bins matching the distribution of the kernel in z. This series of Gaussian pulses represents the probability of a bin containing a PV.  % TODO: Mention variable width in plans
PVs with at least 5 detectable tracks within the LHCb detector acceptance are called LHCb PVs.

\begin{figure}
	\centering
	\input{kernel}
	\caption{The kernel for a simple three track system. 4000 planes in z (simplified in diagram) are computed to make the kernel along z (bottom).}
	\label{fig:kernel}
\end{figure}

\section{Network Design}

\begin{figure}
	\centering
	\input{nnarch}
	\caption{The final network architecture used for the results section. This network has four convolutional layers. For training, dropout was introduced in-between the layers, and at the beginning of the training the final layer was a Fully Connected (FC) layer, and was replaced later in the training procedure.}
	\label{fig:nnarch}
\end{figure}

The network was designed primarily using convolutional layers. The most successful network to date is shown in Figure \ref{fig:nnarch}. The widths of each convolution were chosen based on visual inspection of the data; the number of channels were increased until benefits were no longer noticeable upon adding new channels. The activation function in-between each hidden layer is a leaky ReLU.

The final activation layer was initially a Sigmoid, since this can only produce values between 0 and 1, much like one would expect for a probability. However, this produced poorly shaped final probability distributions, with a distinctly square shape capped near 1, rather than Gaussian, and regularly over-estimated the probability. After changing to the Softplus function, the output probability distributions were much closer to the target. The flat derivative of the Sigmoid for large values is likely to blame for the inability of the network to correct a value that is too large.

The loss function was originally designed to be symmetric, that is, to ``punish'' the network for adding a probability where there was a zero in the target in exactly amount as not adding a probability when there was one in the target. This should be symmetric with respect to  $r \equiv \frac{\hat y}{y}$ and $r^{-1}$, where $y$ is the predicted value and $\hat y$ is the target value. It should be similar to cross entropy, which is commonly defined as
%
$
\mathrm{cost} = - \left(
y \ln \hat y + (1-y) \ln (1 - \hat y)
\right)
$.
%
Our original ``symmetric'' cost function therefore was defined as
\begin{eqnarray}
r_i & \equiv & \frac{\hat y_i + \epsilon}{y_i + \epsilon}, \\
z_i & \equiv & \frac{2 r_i}{r_i + r_i^{-1}}, \\
\mathrm{cost}  & \equiv &- \sum_\mathrm{bins} \ln z_i.
\end{eqnarray}
%
The extra parameter here, $\epsilon$, is just a small parameter for stability that controls the scale of 0 in this cost.

This loss function was found to highly favor minimal false positives (FP) at the expense of efficiency. A single asymmetry parameter was added to control the cost function, and provides a powerful control for selecting the FP to efficiency tradeoff. This loss function can be constructed from the other one with an asymmetry term $a$ and then
\begin{equation}
z_i' = z_i \left(
1+a e^{-r_i}
\right).
\end{equation}
Then $z_i'$ is used in place of $z_i$.
Figure \ref{fig:loss} shows the response for several cost functions, in both symmetric and asymmetric forms.


\begin{figure}
	\centering
	\includegraphics[width=\textwidth]{images/LossPaper.pdf}
	\caption{Plots of the loss function vs. $\hat y$ for several value of $y$. Log scale on the left, linear in the middle, and a reduced range linear plot on the right. Darker lines correspond to the asymmetric cost function. $\epsilon=10^{-5}$, and $a=5.0$.}
	\label{fig:loss}
\end{figure}

One of the most important additions to the original network was ``masking''. Some PVs do not pass our criterion for a good, detectable LHCb PV due to a lack of detectable tracks. These PVs were given a masked region roughly equal to the width of the PV Gaussian probability in the target kernel, and the cost function skips these regions. This keeps the training from punishing or rewarding discovery in these regions. All further calculations, such as efficiency, also ignore these masked regions.

\section{Results}

Our current results are shown in Figure \ref{fig:results}. This shows an efficiency of 88\% for the symmetric cost function, and almost 94\% for asymmetric cost function. Above 20 long LHCb tracks, there is nearly 100\% efficiency. False positive rates here are reported per-event.

\begin{figure}
	\centering
	\includegraphics[width=\textwidth]{images/effntrackspaper.pdf}
	\caption{Results from training a network with a symmetric and an asymmetric cost function (with $a=5$). The upper curve was trained the asymmetric cost function. The lower plot is a histogram of the number of PVs over the number tracks within the LHCb acceptance. Log scale shown on the right, where the points are almost linear.}
    \label{fig:results}
\end{figure}

In Figure \ref{fig:efffp}, several different values of $a$ were used in training, up to $a=5.0$, as well as a symmetric training (equivalent to $a=0$). This shows the asymmetry parameter can be used as a tuning parameter to control this trade-off between efficiency and false positive rate.

\begin{figure}
	\centering
	\includegraphics[width=.8\textwidth]{images/EffVsFP2paper.pdf}
	\caption{False positive rate vs. efficiency for several values of asymmetry term $a$.}
	\label{fig:efffp}
\end{figure}

\section{Plans}

While the initial study has been effective and has shown exciting results, there are several planned improvements.

We are storing the point in $x$ and $y$ where the maximum occurred. This information could be included in the network to give the model more to learn from. The current problem with this is that the network overestimates the importance of these values compared to $z$ and stalls. This information could also be used to predict all three coordinates of the PV instead of just $z$; early studies on this have been promising.

This work has been focused so far on a 1D kernel; however, a 2D kernel is also a possibility. 1D looks to be sufficient for the upgrade conditions for LHCb, but other detectors or much higher pileup could create higher congestion in the 1D kernel which should be dramatically reduced in 2D. The kernel would be binned in one more coordinate, such as $x$ or $y$.

The next project will be to split the prototracking out from the kernel generation in our software, to allow the tracks to be input from the official LHCb simulation. There is work ongoing to do this, as well as to implement the inference engine in the LHCb trigger system.

\section*{References}

\bibliography{pvfinder}

\end{document}


